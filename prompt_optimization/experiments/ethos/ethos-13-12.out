{"task": "ethos", "data_dir": "data/ethos", "prompts": "prompts/ethos.md", "out": "experiments/ethos.out", "max_threads": 32, "temperature": 0.0, "optimizer": "nl-gradient", "rounds": 6, "beam_size": 4, "n_test_exs": 400, "minibatch_size": 64, "n_gradients": 4, "errors_per_gradient": 4, "gradients_per_error": 1, "steps_per_gradient": 1, "mc_samples_per_step": 4, "max_expansion_factor": 8, "engine": "chatgpt", "evaluator": "bf", "scorer": "01", "eval_rounds": 8, "eval_prompts_per_round": 8, "samples_per_eval": 32, "c": 1.0, "knn_k": 2, "knn_t": 0.993, "reject_on_errors": true, "reflect_gradients": false, "reflect_candidates": false, "reflection_candidate_threshold": 0.5, "reflection_gradient_passes": 1, "reflection_candidate_passes": 1, "reflection_temperature": 0.0, "ea_samples_per_step": 4, "eval_budget": 2048}
======== ROUND 0
Time: 5.218885898590088
Prompt: ('# Task\nIs the following text hate speech?\n\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:',)
Training accuracy: (np.float64(0.87),)
Test accuracy: [0.8666666666666667]
======== ROUND 1
Time: 88.70300078392029
Prompt: ('# Task\nYou are a zero‑shot classifier whose job is to spot hate speech. Use the following decision flow:\n\n1. **Is a protected group referenced?**  \n   *Protected groups = race, religion, nationality, gender, sexual orientation, disability, etc.*  \n   - **If not** → label **No**.  \n   - **If yes** → proceed to step 2.\n\n2. **Is there a slur, demeaning label, or violent threat aimed at that group?**  \n   - **If yes** → label **Yes**.  \n   - **If no** → proceed to step 3.\n\n3. **Does the text contain harassing or humiliating language directed at that group (e.g., “fat bitch”, “dump him back”, “STUPID they are”), even without a slur?**  \n   - **If yes** → label **Yes**.  \n   - **If no** → label **No**.\n\n**Note:** General profanity or negative remarks that are not targeted at a protected group do **not** count as hate speech.\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs the following text hate speech?\n\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are an AI trained for content moderation.  \nYour job: determine whether a given sentence or short paragraph constitutes **hate speech**.  \nFollow these rules:\n\n- **Hate speech**: content that *explicitly or implicitly* discriminates against, degrades, or threatens a protected group.  \n- **Protected groups**: race, ethnicity, nationality, gender, sexual orientation, religion, disability, age, and other protected categories.  \n- **Harassing language**: includes derogatory remarks, dehumanizing terms, or threats directed at a group.  \n- **Neutral or non‑targeted insults**: general profanity or criticism that does not mention a protected group is **not** hate speech.\n\nRespond solely with `Yes` or `No`. Do not provide any explanation.\n\n**Example**  \nText: “Deport him in military aircraft and dump him back where he is originated from. Him and his family”  \nLabel: No\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a zero‑shot classifier whose job is to spot hate speech.  \nUse the following decision flow:\n\n1. **Does the text reference a protected group?** (e.g., race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → Output **No**.  \n   - If **Yes** → Proceed to step 2.\n\n2. **Does the text contain a slur, demeaning descriptor, or an explicit violent threat directed at that group?**  \n   - If **Yes** → Output **Yes**.  \n   - If **No** → Proceed to step 3.\n\n3. **Does the text employ harassing or humiliating language aimed at that group** (such as “fat bitch”, “dump him back”, “STUPID they are”), even when no slur is used?  \n   - If **Yes** → Output **Yes**.  \n   - If **No** → Output **No**.\n\n**Note:** General profanity or negative opinions that are not targeted at a protected group do **not** count as hate speech.\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.88), np.float64(0.87), np.float64(0.87), np.float64(0.86))
Test accuracy: [0.9, 0.8733333333333333, 0.8666666666666667, 0.8933333333333333]
======== ROUND 2
Time: 409.41729712486267
Prompt: ('# Task\n**You are a zero‑shot classifier tasked with detecting hate speech. Follow this decision tree:**\n\n1. **Is the passage directed at a protected group?**  \n   (Examples: race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → write **No**.  \n   - If **Yes** → go to step\u202f2.\n\n2. **Does the passage contain a slur, a demeaning term, or an explicit violent threat aimed at that group?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → go to step\u202f3.\n\n3. **Does the passage use harassing or humiliating language toward that group—such as “fat bitch,” “dump him back,” “STUPID they are”—even without a slur?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → write **No**.\n\n> **Important:** General profanity or negative opinions not targeting a protected group do **not** constitute hate speech.\n\n---\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: **Yes**\n\n---\n\n**Now classify the following text:**\n\nText: {TEXT_HERE}\n\nLabel:\n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a zero‑shot classifier whose job is to spot hate speech. Use the following decision flow:\n\n1. **Is a protected group referenced?**  \n   *Protected groups = race, religion, nationality, gender, sexual orientation, disability, etc.*  \n   - **If not** → label **No**.  \n   - **If yes** → proceed to step 2.\n\n2. **Is there a slur, demeaning label, or violent threat aimed at that group?**  \n   - **If yes** → label **Yes**.  \n   - **If no** → proceed to step 3.\n\n3. **Does the text contain harassing or humiliating language directed at that group (e.g., “fat bitch”, “dump him back”, “STUPID they are”), even without a slur?**  \n   - **If yes** → label **Yes**.  \n   - **If no** → label **No**.\n\n**Note:** General profanity or negative remarks that are not targeted at a protected group do **not** count as hate speech.\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a hate‑speech detection model. For this task, hate speech is any statement that:\n\n- **Targets a protected group** (e.g., race, religion, ethnicity, sexual orientation, gender identity, disability, nationality, or any other protected characteristic), **and**\n- **Includes a threat, insult, or a call for harm or exclusion** directed at that group.\n\nIf either of these conditions is not met, the text is **not** hate speech.\n\nNow evaluate the following passage:\n\nText: {TEXT}\n\nRespond **Yes** if it qualifies as hate speech, otherwise respond **No**. Only provide the single word answer.\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a zero‑shot classifier tasked with identifying hate speech.  \nFollow this narrative decision process:\n\n1. **Identify the target** – Does the text explicitly refer to a protected group (race, religion, gender, etc.)?  \n   - If *not*, the answer is **No**.\n\n2. **Check for explicit hate** – Is there a slur or a direct threat of violence against that group?  \n   - If *yes*, the answer is **Yes**.\n\n3. **Check for harassing language** – Is the text harassing, demeaning, or humiliating toward that group, even without a slur or threat?  \n   - If *yes*, the answer is **Yes**.\n\n4. **Otherwise** – The answer is **No**.\n\n**Guidelines**  \n- General swearing not aimed at a protected group → **No**.  \n- Ambiguous language should be treated as **No** unless a protected group is clearly targeted.  \n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.9), np.float64(0.88), np.float64(0.88), np.float64(0.87))
Test accuracy: [0.8866666666666667, 0.88, 0.8666666666666667, 0.82]
======== ROUND 3
Time: 383.4127793312073
Prompt: ('# Task\n**You are a zero‑shot classifier tasked with detecting hate speech. Follow this decision tree:**\n\n1. **Is the passage directed at a protected group?**  \n   (Examples: race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → write **No**.  \n   - If **Yes** → go to step\u202f2.\n\n2. **Does the passage contain a slur, a demeaning term, or an explicit violent threat aimed at that group?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → go to step\u202f3.\n\n3. **Does the passage use harassing or humiliating language toward that group—such as “fat bitch,” “dump him back,” “STUPID they are”—even without a slur?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → write **No**.\n\n> **Important:** General profanity or negative opinions not targeting a protected group do **not** constitute hate speech.\n\n---\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: **Yes**\n\n---\n\n**Now classify the following text:**\n\nText: {TEXT_HERE}\n\nLabel:\n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Task:**  \nYou are a zero‑shot classifier. Your job is to decide whether a given text contains **hate speech**.  \nRespond with **Yes** (hate speech) or **No** (not hate speech).\n\n---\n\n### Procedure\n\n1. **Look for a protected group**  \n   * Does the text reference, explicitly or implicitly, a protected group (race, ethnicity, nationality, religion, gender, sexual orientation, disability, etc.)?  \n   * If no protected group is identifiable, jump to step\u202f3.\n\n2. **Check for explicit or implicit hostility**  \n   a. **Explicit hostility** – Is there a slur, a direct threat of violence, or a statement encouraging violence or persecution against that group?  \n      *If yes, answer **Yes**. If no, continue to 2b.*  \n   b. **Implicit hostility** – Does the text use demeaning, humiliating, or harassing language toward that group that is clearly hostile even without a slur or threat?  \n      *If yes, answer **Yes**. If no, go to step\u202f3.*\n\n3. **Assess contextual or indirect hate**  \n   * Does the text contain strongly negative language (e.g., “kill them,” “destroy them,” “they’re a plague”) directed at people who might belong to a protected group, even if the group isn’t named?  \n   * Does it employ derogatory stereotypes or generalized insults that target a protected group (e.g., “they’re all bad,” “they’re lazy”) without naming the group?  \n   *If either question is answered **yes**, answer **Yes**; otherwise, answer **No**.*\n\n---\n\n### Quick Rules\n\n- General profanity aimed at an individual or an unspecified group → **No**.  \n- Vague or ambiguous statements that can be interpreted in multiple ways → **No**, unless a protected group is clearly implicated.  \n- If a protected group is referenced only indirectly but the hateful intent is obvious from context → **Yes**.  \n- Presence of slurs, explicit threats, calls for violence, or demeaning stereotypes about a protected group is a strong indicator of hate speech.\n\n---\n\n### Example Decisions\n\n| Text | Decision |\n|------|----------|\n| “fat bitch....she need that black cucumber up her tight u know what....lmfaooo” | Yes |\n| “Humiliating this Str8 guy. str8 guys like him doing dumb things” | No |\n| “Pale cancer! Back to your west pacific ocean” | Yes |\n| “Didn’t know shitted coloured like you could talk” | Yes |\n| “hahahahahahahahaha europ fuck you fucking nazis” | Yes |\n\n---\n\n**Now classify the following text:**\n\nText: `{TEXT_HERE}`\n\nLabel: \n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a model that detects hate speech. For this task, a statement is considered hate speech only if it:\n\n- **Targets a protected group** (such as race, religion, ethnicity, sexual orientation, gender identity, disability, nationality, or any other protected characteristic), **and**\n- **Contains a threat, insult, or a call for harm or exclusion** aimed at that group.\n\nIf either condition is missing, the text is **not** hate speech.\n\nNow assess the following passage:\n\nText: {TEXT}\n\nAnswer **Yes** if it meets the hate‑speech criteria, otherwise answer **No**. Respond with only the single word.\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a zero‑shot classifier whose job is to spot hate speech. Follow the expanded decision tree below:\n\n1. **Protected‑Group Mention** – Does the passage refer to a protected group (race, religion, ethnicity, nationality, gender, sexual orientation, disability, etc.)?  \n   - If *yes*, go to step\u202f2.  \n   - If *no*, go to step\u202f3.\n\n2. **Explicit Hate** – Does the text contain a slur, a direct threat of violence, or a statement that the group is inherently bad or dangerous?  \n   - If *yes*, **Label: Yes**.  \n   - If *no*, go to step\u202f4.\n\n3. **Dehumanizing / Degrading Terms** – Does the text use words that reduce the group to animals, vermin, criminals, or suggest they must be removed or exterminated, even if no slur is present?  \n   - If *yes*, **Label: Yes**.  \n   - If *no*, go to step\u202f4.\n\n4. **Harassment / Demeaning** – Does the text harass, insult, or threaten the group (or an implied group) in a way that encourages violence or social exclusion?  \n   - If *yes*, **Label: Yes**.  \n   - If *no*, **Label: No**.\n\n5. **No Protected‑Group Mention** – If the passage does not name a protected group but includes violent or demeaning language aimed at a broad category (e.g., “they’re a problem in our city”), label it **Yes** only if it clearly targets a group.\n\n**Guidelines**  \n- Swearing at an individual without a protected group → **No**.  \n- Vague statements that could apply to anyone → **No**.  \n- Any statement that frames an entire group as a threat, blames them for societal problems, or calls for their removal/violence → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.9), np.float64(0.89), np.float64(0.88), np.float64(0.88))
Test accuracy: [0.8733333333333333, 0.8733333333333333, 0.8333333333333334, 0.8666666666666667]
======== ROUND 4
Time: 453.0144155025482
Prompt: ('# Task\n**Revised Instruction**\n\nYou are a zero‑shot classifier whose job is to determine whether a given passage contains hate speech.  \nRespond with a single word: **Yes** or **No**.\n\nApply the decision flow below, taking into account the *context* and *intent* of the text.\n\n1. **Does the passage mention a protected group?**  \n   - **No**:  \n     a. Is there a violent threat or harassing language directed at a *specific* group (even if the group isn’t named)? → **Yes**.  \n     b. If not → **No**.  \n   - **Yes**: proceed to step\u202f2.\n\n2. **Is there explicit hate?**  \n   - Presence of a slur, a direct threat, or a statement that the group is inherently bad or dangerous → **Yes**.  \n   - Otherwise → step\u202f3.\n\n3. **Is the language dehumanizing or degrading?**  \n   - Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.  \n   - Otherwise → step\u202f4.\n\n4. **Is there harassment or demeaning content?**  \n   - Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.  \n   - Otherwise → step\u202f5.\n\n5. **Does the passage praise or glorify violent extremist conduct?**  \n   - Praise for a violent individual or extremist ideology → **Yes**.  \n   - Otherwise → **No**.\n\n**Special Considerations**\n\n- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.  \n- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.  \n- Vague statements that could apply to anyone → **No** unless they target a protected group.  \n- Statements that frame an entire group as a societal problem or call for their removal → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\n**Task**  \nClassify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**You are a zero‑shot classifier tasked with detecting hate speech. Follow this decision tree:**\n\n1. **Is the passage directed at a protected group?**  \n   (Examples: race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → write **No**.  \n   - If **Yes** → go to step\u202f2.\n\n2. **Does the passage contain a slur, a demeaning term, or an explicit violent threat aimed at that group?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → go to step\u202f3.\n\n3. **Does the passage use harassing or humiliating language toward that group—such as “fat bitch,” “dump him back,” “STUPID they are”—even without a slur?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → write **No**.\n\n> **Important:** General profanity or negative opinions not targeting a protected group do **not** constitute hate speech.\n\n---\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: **Yes**\n\n---\n\n**Now classify the following text:**\n\nText: {TEXT_HERE}\n\nLabel:\n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Objective:**  \nYou are a zero‑shot classifier tasked with determining whether a piece of text is **hate speech**. Return only **“Yes”** if the text contains hate speech, otherwise return **“No.”**\n\n---\n\n### Decision Flow\n\n1. **Identify a protected group**  \n   - Does the text mention (directly or indirectly) a protected group such as race, ethnicity, nationality, religion, gender, sexual orientation, disability, etc.?  \n   - If no protected group is discernible, skip to step\u202f3.\n\n2. **Look for hostility toward that group**  \n   a. **Explicit hostility** – Presence of a slur, a direct threat of violence, or a call to violence/persecution against the group.  \n   - If found, answer **Yes**.  \n   b. **Implicit hostility** – Demeaning, humiliating, or harassing language that is clearly hostile even without a slur or threat.  \n   - If found, answer **Yes**.  \n   - If neither, proceed to step\u202f3.\n\n3. **Evaluate contextual or indirect hate**  \n   - Does the text use strongly negative language (e.g., “kill them,” “destroy them,” “they’re a plague”) aimed at people who may belong to a protected group, even if the group isn’t named?  \n   - Does it employ derogatory stereotypes or generalized insults that target a protected group without naming it?  \n   - If either is true, answer **Yes**; otherwise, answer **No**.\n\n---\n\n### Quick Reference Rules\n\n- General profanity aimed at an individual or an unspecified group → **No**.  \n- Vague or ambiguous statements that could be interpreted in many ways → **No**, unless a protected group is clearly implicated.  \n- If a protected group is mentioned only indirectly but the hateful intent is unmistakable from context → **Yes**.  \n- Presence of slurs, explicit threats, calls for violence, or demeaning stereotypes about a protected group strongly indicates hate speech.\n\n---\n\n### Sample Decisions\n\n| Text | Decision |\n|------|----------|\n| “fat bitch....she need that black cucumber up her tight u know what....lmfaooo” | Yes |\n| “Humiliating this Str8 guy. str8 guys like him doing dumb things” | No |\n| “Pale cancer! Back to your west pacific ocean” | Yes |\n| “Didn’t know shitted coloured like you could talk” | Yes |\n| “hahahahahahahahaha europ fuck you fucking nazis” | Yes |\n\n---\n\n**Now classify the following text:**\n\nText: `{TEXT_HERE}`\n\nLabel:  \n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Revised Instruction**\n\n**Task**  \nYou are a zero‑shot classifier. Your role is to determine whether a given passage contains **hate speech**—that is, language that is hostile, demeaning, or encourages violence toward a protected group.  \nReply with **Yes** if the text is hate speech, otherwise **No**.\n\n**Procedure**\n\n1. **Detect the target(s)**  \n   - Scan for any mention of a protected group: race, ethnicity, nationality, religion, gender, sexual orientation, disability, etc.  \n   - If the group is only implied (e.g., “the Israelis”, “Europeans”, “coloured people”), treat it as a protected group.  \n   - If no protected group is identifiable, proceed to step\u202f3; the absence of a name does **not** automatically mean “No”.\n\n2. **Evaluate hostility**  \n   - **Explicit hostility**: slurs, direct threats, calls for violence, or encouragement of persecution.  \n   - **Implicit hostility**: demeaning, humiliating, or harassing language that is clearly hostile even without a slur or threat.  \n   - If either type of hostility is present, answer **Yes**.\n\n3. **Contextual/Hypothetical Hate**  \n   - If the text contains strongly negative, violent, or demeaning language toward a group that is *not named but is inferred from context* (e.g., “kill them”, “destroy them”, “they’re a plague”), answer **Yes**.  \n   - If the text only criticizes a political stance or policy without any negative language about a protected group, answer **No**.\n\n**Quick Decision Rules**\n\n- General profanity aimed at an unspecified individual → **No**.  \n- Vague statements that could apply to many groups → **No**, unless context clarifies a protected group.  \n- Presence of a protected group (named or implied) plus any hostility criterion → **Yes**.  \n- Explicit slurs, threats, or violent language toward a protected group → **Yes**.\n\n**Illustrative Examples**\n\n| Text | Decision |\n|------|----------|\n| “Easter is the most silly holiday ever existed. It must be stopped. Shame shame shame” | No |\n| “Didn’t know shitted coloured like you could talk” | Yes |\n| “Magnificient. Contrary to what the media reports, I have hope for the people of Europe to expel these leeches.” | Yes |\n| “Seen the way the Israelis treat the Palestinians? Slaughter on a weekly basis, very racist are the Israelis to the point of genocide.” | Yes |\n| “Humiliating this Str8 guy. str8 guys like him doing dumb things” | No |\n\n**Now classify the following text:**\n\nText: `{TEXT_HERE}`  \n\n**Label:**  \n\n**Output:**\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.9), np.float64(0.9), np.float64(0.9), np.float64(0.9))
Test accuracy: [0.84, 0.8866666666666667, 0.8733333333333333, 0.8666666666666667]
======== ROUND 5
Time: 463.15204882621765
Prompt: ('# Task\n**Revised Instruction**\n\nYou are a zero‑shot classifier tasked with determining whether a passage contains hate speech.  \nReply with a single word: **Yes** or **No**.\n\nFollow this decision tree, keeping in mind the *context* and *intent* of the text.\n\n1. **Is a protected group referenced?**  \n   - **No**:  \n     a. Does the text contain a violent threat or harassing language aimed at a *specific* group (even if the group isn’t named?) → **Yes**.  \n     b. If not → **No**.  \n   - **Yes**: move to step\u202f2.\n\n2. **Is explicit hate present?**  \n   - Presence of a slur, a direct threat, or a statement that members of the group are inherently bad or dangerous → **Yes**.  \n   - Otherwise → step\u202f3.\n\n3. **Is the language dehumanizing or demeaning?**  \n   - Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.  \n   - Otherwise → step\u202f4.\n\n4. **Is there harassment or demeaning content?**  \n   - Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.  \n   - Otherwise → step\u202f5.\n\n5. **Does the passage praise or glorify violent extremist conduct?**  \n   - Praise for a violent individual or extremist ideology → **Yes**.  \n   - Otherwise → **No**.\n\n**Special Cases**\n\n- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.  \n- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.  \n- Vague statements that could apply to anyone → **No** unless they target a protected group.  \n- Statements that portray an entire group as a societal problem or call for their removal → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\n**Task**  \nClassify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Revised Instruction**\n\nYou are a zero‑shot classifier whose job is to determine whether a given passage contains hate speech.  \nRespond with a single word: **Yes** or **No**.\n\nApply the decision flow below, taking into account the *context* and *intent* of the text.\n\n1. **Does the passage mention a protected group?**  \n   - **No**:  \n     a. Is there a violent threat or harassing language directed at a *specific* group (even if the group isn’t named)? → **Yes**.  \n     b. If not → **No**.  \n   - **Yes**: proceed to step\u202f2.\n\n2. **Is there explicit hate?**  \n   - Presence of a slur, a direct threat, or a statement that the group is inherently bad or dangerous → **Yes**.  \n   - Otherwise → step\u202f3.\n\n3. **Is the language dehumanizing or degrading?**  \n   - Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.  \n   - Otherwise → step\u202f4.\n\n4. **Is there harassment or demeaning content?**  \n   - Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.  \n   - Otherwise → step\u202f5.\n\n5. **Does the passage praise or glorify violent extremist conduct?**  \n   - Praise for a violent individual or extremist ideology → **Yes**.  \n   - Otherwise → **No**.\n\n**Special Considerations**\n\n- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.  \n- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.  \n- Vague statements that could apply to anyone → **No** unless they target a protected group.  \n- Statements that frame an entire group as a societal problem or call for their removal → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\n**Task**  \nClassify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**You are a zero‑shot classifier tasked with detecting hate speech. Follow this decision tree:**\n\n1. **Is the passage directed at a protected group?**  \n   (Examples: race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → write **No**.  \n   - If **Yes** → go to step\u202f2.\n\n2. **Does the passage contain a slur, a demeaning term, or an explicit violent threat aimed at that group?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → go to step\u202f3.\n\n3. **Does the passage use harassing or humiliating language toward that group—such as “fat bitch,” “dump him back,” “STUPID they are”—even without a slur?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → write **No**.\n\n> **Important:** General profanity or negative opinions not targeting a protected group do **not** constitute hate speech.\n\n---\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: **Yes**\n\n---\n\n**Now classify the following text:**\n\nText: {TEXT_HERE}\n\nLabel:\n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Objective:**  \nYou are a zero‑shot classifier tasked with determining whether a piece of text is **hate speech**. Return only **“Yes”** if the text contains hate speech, otherwise return **“No.”**\n\n---\n\n### Decision Flow\n\n1. **Identify a protected group**  \n   - Does the text mention (directly or indirectly) a protected group such as race, ethnicity, nationality, religion, gender, sexual orientation, disability, etc.?  \n   - If no protected group is discernible, skip to step\u202f3.\n\n2. **Look for hostility toward that group**  \n   a. **Explicit hostility** – Presence of a slur, a direct threat of violence, or a call to violence/persecution against the group.  \n   - If found, answer **Yes**.  \n   b. **Implicit hostility** – Demeaning, humiliating, or harassing language that is clearly hostile even without a slur or threat.  \n   - If found, answer **Yes**.  \n   - If neither, proceed to step\u202f3.\n\n3. **Evaluate contextual or indirect hate**  \n   - Does the text use strongly negative language (e.g., “kill them,” “destroy them,” “they’re a plague”) aimed at people who may belong to a protected group, even if the group isn’t named?  \n   - Does it employ derogatory stereotypes or generalized insults that target a protected group without naming it?  \n   - If either is true, answer **Yes**; otherwise, answer **No**.\n\n---\n\n### Quick Reference Rules\n\n- General profanity aimed at an individual or an unspecified group → **No**.  \n- Vague or ambiguous statements that could be interpreted in many ways → **No**, unless a protected group is clearly implicated.  \n- If a protected group is mentioned only indirectly but the hateful intent is unmistakable from context → **Yes**.  \n- Presence of slurs, explicit threats, calls for violence, or demeaning stereotypes about a protected group strongly indicates hate speech.\n\n---\n\n### Sample Decisions\n\n| Text | Decision |\n|------|----------|\n| “fat bitch....she need that black cucumber up her tight u know what....lmfaooo” | Yes |\n| “Humiliating this Str8 guy. str8 guys like him doing dumb things” | No |\n| “Pale cancer! Back to your west pacific ocean” | Yes |\n| “Didn’t know shitted coloured like you could talk” | Yes |\n| “hahahahahahahahaha europ fuck you fucking nazis” | Yes |\n\n---\n\n**Now classify the following text:**\n\nText: `{TEXT_HERE}`\n\nLabel:  \n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.9), np.float64(0.9), np.float64(0.9), np.float64(0.9))
Test accuracy: [0.8266666666666667, 0.8533333333333334, 0.8733333333333333, 0.88]
======== ROUND 6
Time: 455.33698987960815
Prompt: ('# Task\n**Revised Instruction**\n\nYou are a zero‑shot classifier tasked with determining whether a passage contains hate speech.  \nReply with a single word: **Yes** or **No**.\n\nFollow this decision tree, keeping in mind the *context* and *intent* of the text.\n\n1. **Is a protected group referenced?**  \n   - **No**:  \n     a. Does the text contain a violent threat or harassing language aimed at a *specific* group (even if the group isn’t named?) → **Yes**.  \n     b. If not → **No**.  \n   - **Yes**: move to step\u202f2.\n\n2. **Is explicit hate present?**  \n   - Presence of a slur, a direct threat, or a statement that members of the group are inherently bad or dangerous → **Yes**.  \n   - Otherwise → step\u202f3.\n\n3. **Is the language dehumanizing or demeaning?**  \n   - Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.  \n   - Otherwise → step\u202f4.\n\n4. **Is there harassment or demeaning content?**  \n   - Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.  \n   - Otherwise → step\u202f5.\n\n5. **Does the passage praise or glorify violent extremist conduct?**  \n   - Praise for a violent individual or extremist ideology → **Yes**.  \n   - Otherwise → **No**.\n\n**Special Cases**\n\n- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.  \n- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.  \n- Vague statements that could apply to anyone → **No** unless they target a protected group.  \n- Statements that portray an entire group as a societal problem or call for their removal → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\n**Task**  \nClassify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**Revised Instruction**\n\nYou are a zero‑shot classifier whose job is to determine whether a given passage contains hate speech.  \nRespond with a single word: **Yes** or **No**.\n\nApply the decision flow below, taking into account the *context* and *intent* of the text.\n\n1. **Does the passage mention a protected group?**  \n   - **No**:  \n     a. Is there a violent threat or harassing language directed at a *specific* group (even if the group isn’t named)? → **Yes**.  \n     b. If not → **No**.  \n   - **Yes**: proceed to step\u202f2.\n\n2. **Is there explicit hate?**  \n   - Presence of a slur, a direct threat, or a statement that the group is inherently bad or dangerous → **Yes**.  \n   - Otherwise → step\u202f3.\n\n3. **Is the language dehumanizing or degrading?**  \n   - Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.  \n   - Otherwise → step\u202f4.\n\n4. **Is there harassment or demeaning content?**  \n   - Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.  \n   - Otherwise → step\u202f5.\n\n5. **Does the passage praise or glorify violent extremist conduct?**  \n   - Praise for a violent individual or extremist ideology → **Yes**.  \n   - Otherwise → **No**.\n\n**Special Considerations**\n\n- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.  \n- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.  \n- Vague statements that could apply to anyone → **No** unless they target a protected group.  \n- Statements that frame an entire group as a societal problem or call for their removal → **Yes**.\n\n**Example**  \nText: “All refugees are a burden and we must keep them out.”  \nLabel: Yes\n\n**Task**  \nClassify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\n**You are a zero‑shot classifier tasked with detecting hate speech. Follow this decision tree:**\n\n1. **Is the passage directed at a protected group?**  \n   (Examples: race, religion, nationality, gender, sexual orientation, disability, etc.)  \n   - If **No** → write **No**.  \n   - If **Yes** → go to step\u202f2.\n\n2. **Does the passage contain a slur, a demeaning term, or an explicit violent threat aimed at that group?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → go to step\u202f3.\n\n3. **Does the passage use harassing or humiliating language toward that group—such as “fat bitch,” “dump him back,” “STUPID they are”—even without a slur?**  \n   - If **Yes** → write **Yes**.  \n   - If **No** → write **No**.\n\n> **Important:** General profanity or negative opinions not targeting a protected group do **not** constitute hate speech.\n\n---\n\n**Example**  \nText: “fat bitch....she need that black cucumber up her tight u know what....lmfaooo”  \nLabel: **Yes**\n\n---\n\n**Now classify the following text:**\n\nText: {TEXT_HERE}\n\nLabel:\n\nOutput:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nYou are a zero‑shot classifier.  \nYour task: determine if the provided text contains **hate speech**.  \nReturn only **“Yes”** if it does, otherwise **“No.”**\n\n**Decision Principles**\n\n1. **Identify a protected group** (race, ethnicity, nationality, religion, gender, sexual orientation, disability, etc.).  \n   • If no protected group is discernible, skip to step\u202f4.  \n\n2. **Check for explicit hostility** toward that group:  \n   a. Slurs or epithets.  \n   b. Direct threats of violence or calls to persecution.  \n   If found → **Yes**.  \n\n3. **Check for implicit hostility**:  \n   • Demeaning, humiliating, or harassing language that is clearly hostile toward the group, even without a slur or threat.  \n   If found → **Yes**.  \n\n4. **Consider contextual hate** (if no protected group was named in step\u202f1 but the text still targets a protected group):  \n   a. Strongly negative, violent, or demeaning language aimed at a group that is implied or stereotyped (e.g., “kill them,” “they’re a plague”).  \n   b. Stereotypical insults or generalized demeaning remarks that are widely understood to target a protected group.  \n   If either → **Yes**.  \n\n5. **All other cases** → **No**.  \n\n**Quick Rules**\n\n- General profanity toward an individual or an unspecified group → **No**.  \n- Vague statements that could mean many things → **No** unless a protected group is clearly implicated.  \n- If a protected group is only implied but the hateful intent is unmistakable → **Yes**.  \n- Presence of slurs, explicit threats, or violent calls strongly indicates hate speech.\n\n**Example**\n\nText: "Deport him in military aircraft and dump him back where he is originated from. Him and his family"  \nDecision: Yes\n\nText: "it\'s a good thing I\'m not a commander or so in the police force, especially not during something like this, because the jail would be full of people waiting to be deported, if not dead"  \nDecision: No\n\nText: "seen the way the israelis treat the palestinians? slaughter on a weekly basis, very racist are the israelis to the point of genocide."  \nDecision: Yes\n\nNow classify the following text:\n\nText: {TEXT_HERE}\n\nLabel:\n# Output format\nAnswer ONLY "Yes" or "No".\nNO explanations.\nNo other words.\n\n# Prediction\nText: {{ text }}\nLabel:')
Training accuracy: (np.float64(0.9), np.float64(0.9), np.float64(0.9), np.float64(0.9))
Test accuracy: [0.8333333333333334, 0.84, 0.8933333333333333, 0.86]
======== EXEMPLAR OPTIMIZATION ROUND 0
Time: 41.59059429168701
Prompt: # Task
**Revised Instruction**

You are a zero‑shot classifier tasked with determining whether a passage contains hate speech.
Reply with a single word: **Yes** or **No**.

Follow this decision tree, keeping in mind the *context* and *intent* of the text.

1. **Is a protected group referenced?**
- **No**:
a. Does the text contain a violent threat or harassing language aimed at a *specific* group (even if the group isn’t named?) → **Yes**.
b. If not → **No**.
- **Yes**: move to step 2.

2. **Is explicit hate present?**
- Presence of a slur, a direct threat, or a statement that members of the group are inherently bad or dangerous → **Yes**.
- Otherwise → step 3.

3. **Is the language dehumanizing or demeaning?**
- Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.
- Otherwise → step 4.

4. **Is there harassment or demeaning content?**
- Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.
- Otherwise → step 5.

5. **Does the passage praise or glorify violent extremist conduct?**
- Praise for a violent individual or extremist ideology → **Yes**.
- Otherwise → **No**.

**Special Cases**

- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.
- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.
- Vague statements that could apply to anyone → **No** unless they target a protected group.
- Statements that portray an entire group as a societal problem or call for their removal → **Yes**.

**Example**
Text: “All refugees are a burden and we must keep them out.”
Label: Yes

**Task**
Classify the following text:

Text: {TEXT_HERE}

Label:

# Exemplar
Text: we take everyone, sudanese niggers, egyptian shitskins, palirats, syrian and iraqi scums, they're all dogs that behave like niggers, they cost too much, especially syrians, there is way too many of them here, they are arrogant ungrateful backstabbing apes.
Label: Yes
Text: We have to take our power back as the superior gender. 
Label: Yes
Text: All girls like you should have a pimp to guide them to successful prostitution
Label: Yes
# Output format
Answer ONLY "Yes" or "No".
NO explanations.
No other words.

# Prediction
Text: {{ text }}
Label:
Training accuracy: 0.92
Test accuracy: 0.88
======== EXEMPLAR OPTIMIZATION ROUND 1
Time: 20.79955768585205
Prompt: # Task
**Revised Instruction**

You are a zero‑shot classifier tasked with determining whether a passage contains hate speech.
Reply with a single word: **Yes** or **No**.

Follow this decision tree, keeping in mind the *context* and *intent* of the text.

1. **Is a protected group referenced?**
- **No**:
a. Does the text contain a violent threat or harassing language aimed at a *specific* group (even if the group isn’t named?) → **Yes**.
b. If not → **No**.
- **Yes**: move to step 2.

2. **Is explicit hate present?**
- Presence of a slur, a direct threat, or a statement that members of the group are inherently bad or dangerous → **Yes**.
- Otherwise → step 3.

3. **Is the language dehumanizing or demeaning?**
- Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.
- Otherwise → step 4.

4. **Is there harassment or demeaning content?**
- Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.
- Otherwise → step 5.

5. **Does the passage praise or glorify violent extremist conduct?**
- Praise for a violent individual or extremist ideology → **Yes**.
- Otherwise → **No**.

**Special Cases**

- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.
- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.
- Vague statements that could apply to anyone → **No** unless they target a protected group.
- Statements that portray an entire group as a societal problem or call for their removal → **Yes**.

**Example**
Text: “All refugees are a burden and we must keep them out.”
Label: Yes

**Task**
Classify the following text:

Text: {TEXT_HERE}

Label:

# Exemplar
Text: we take everyone, sudanese niggers, egyptian shitskins, palirats, syrian and iraqi scums, they're all dogs that behave like niggers, they cost too much, especially syrians, there is way too many of them here, they are arrogant ungrateful backstabbing apes.
Label: Yes
Text: We have to take our power back as the superior gender. 
Label: Yes
Text: All girls like you should have a pimp to guide them to successful prostitution
Label: Yes
# Output format
Answer ONLY "Yes" or "No".
NO explanations.
No other words.

# Prediction
Text: {{ text }}
Label:
Training accuracy: 0.92
Test accuracy: 0.88
======== EXEMPLAR OPTIMIZATION ROUND 2
Time: 21.59258270263672
Prompt: # Task
**Revised Instruction**

You are a zero‑shot classifier tasked with determining whether a passage contains hate speech.
Reply with a single word: **Yes** or **No**.

Follow this decision tree, keeping in mind the *context* and *intent* of the text.

1. **Is a protected group referenced?**
- **No**:
a. Does the text contain a violent threat or harassing language aimed at a *specific* group (even if the group isn’t named?) → **Yes**.
b. If not → **No**.
- **Yes**: move to step 2.

2. **Is explicit hate present?**
- Presence of a slur, a direct threat, or a statement that members of the group are inherently bad or dangerous → **Yes**.
- Otherwise → step 3.

3. **Is the language dehumanizing or demeaning?**
- Words that reduce the group to animals, vermin, criminals, or imply extermination → **Yes**.
- Otherwise → step 4.

4. **Is there harassment or demeaning content?**
- Insult, threat, or encouragement of violence/social exclusion against the group → **Yes**.
- Otherwise → step 5.

5. **Does the passage praise or glorify violent extremist conduct?**
- Praise for a violent individual or extremist ideology → **Yes**.
- Otherwise → **No**.

**Special Cases**

- Praise or admiration of a violent extremist (e.g., “I want to be like John who killed 900 people”) → **Yes**, even if no protected group is mentioned.
- Swearing at a non‑protected individual (e.g., “Humiliating this Str8 guy”) → **No**.
- Vague statements that could apply to anyone → **No** unless they target a protected group.
- Statements that portray an entire group as a societal problem or call for their removal → **Yes**.

**Example**
Text: “All refugees are a burden and we must keep them out.”
Label: Yes

**Task**
Classify the following text:

Text: {TEXT_HERE}

Label:

# Exemplar
Text: I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
Label: Yes
Text: hahahahahahahahaha europ fuck you fucking nazis
Label: Yes
Text: If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
Label: Yes
# Output format
Answer ONLY "Yes" or "No".
NO explanations.
No other words.

# Prediction
Text: {{ text }}
Label:
Training accuracy: 0.93
Test accuracy: 0.8866666666666667
